{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-24T15:14:29.051938",
     "start_time": "2016-11-24T15:14:29.010504"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import airflow\n",
    "from airflow.hooks.hive_hooks import HiveCliHook, HiveServer2Hook, HiveMetastoreHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-24T15:14:37.771483",
     "start_time": "2016-11-24T15:14:37.768961"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__version = 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HiveToVWInput(object):\n",
    "    def __init__(self,\n",
    "                 src_table,\n",
    "                 dst_table,\n",
    "                 dst_hdfs_location,\n",
    "                 label_column,\n",
    "                 tag_column,\n",
    "                 limit=None,\n",
    "                 sample_sql=None,\n",
    "                 excludes=None,\n",
    "                 custom_namespaces=None,\n",
    "                 hive_conn_id='hive_silver',\n",
    "                 metastore_conn_id='metastore_silver',\n",
    "                ):\n",
    "        if len(src_table.split('.')) != 2:\n",
    "            raise TypeError('src_table should be of the format namespace.table_name')\n",
    "        else:\n",
    "            (self.src_db, self.src_table_name) = src_table.split('.')\n",
    "        if len(dst_table.split('.')) != 2:\n",
    "            raise TypeError('dst_table should be of the format namespace.table_name')\n",
    "        else:\n",
    "            (self.dst_db, self.dst_table_name) = dst_table.split('.')\n",
    "        self.src_table = src_table\n",
    "        self.dst_table = dst_table\n",
    "        self.dst_hdfs_location = dst_hdfs_location\n",
    "        self.sample_sql = sample_sql\n",
    "        self.label_column = label_column\n",
    "        self.limit = limit\n",
    "        self.hive_conn_id = hive_conn_id\n",
    "        self.metastore_conn_id = metastore_conn_id\n",
    "        self.tag_column = tag_column\n",
    "\n",
    "        # Fetch the table defininition from the hive metastore\n",
    "        hmh = HiveMetastoreHook(metastore_conn_id)\n",
    "        table_def = hmh.get_table(db=self.src_db, table_name=self.src_table_name)\n",
    "        cols = table_def.sd.cols\n",
    "\n",
    "        # Build the VW namespaces from column names\n",
    "        # These are in the format { 'col1': ('ns1', 'datatype'), 'col2': ('ns2', 'datatype') }\n",
    "        nslookup = {}\n",
    "        for c in cols:\n",
    "            if c.name in excludes or c.name == self.label_column:\n",
    "                next\n",
    "            elif c.name in custom_namespaces.keys():\n",
    "                nslookup[c.name] = {'namespace': custom_namespaces[c.name], 'type': c.type }\n",
    "            elif len(c.name.split('__')) > 2:\n",
    "                nslookup[c.name] = {'namespace': c.name.split('__')[1], 'type': c.type }\n",
    "            else:\n",
    "                nslookup[c.name] = {'namespace': 'other', 'type': c.type }\n",
    "        self.nslookup = nslookup\n",
    "                \n",
    "        # Let's build a namespace-keyed dictionary with (colname, dtype) as values\n",
    "        nsgroups = {}\n",
    "        for col in nslookup.keys():\n",
    "            if nslookup[col]['namespace'] not in nsgroups.keys():\n",
    "                nsgroups[nslookup[col]['namespace']] = [(col, nslookup[col]['type'])]\n",
    "            else:\n",
    "                nsgroups[nslookup[col]['namespace']].append((col, nslookup[col]['type']))\n",
    "        self.nsgroups = nsgroups\n",
    "    \n",
    "    def __col_sql(self, colname, coltype):\n",
    "        \"\"\" Return the properly formatted SQL for the particular column name and type\"\"\"\n",
    "        if coltype == 'double':\n",
    "            #return \"CONCAT('{col}:', PRINTF('%.2f', COALESCE({col},0.0)), ' ')\".format(col=colname)\n",
    "            return \"CASE WHEN COALESCE({col},0.0) = 0.0 THEN '' ELSE CONCAT('{col}:', PRINTF('%.2f', {col}), ' ') END\".format(col=colname)\n",
    "        elif coltype in ['bigint', 'int']:\n",
    "            #return \"CONCAT('{col}:', PRINTF('%d', COALESCE({col},0)), ' ')\".format(col=colname)\n",
    "            return \"CASE WHEN COALESCE({col},0) = 0 THEN '' ELSE CONCAT('{col}:', PRINTF('%d', {col}), ' ') END\".format(col=colname)\n",
    "        elif coltype in ['boolean']:\n",
    "            #return \"CONCAT('{col}:', PRINTF('%d', COALESCE({col},0)), ' ')\".format(col=colname)\n",
    "            return \"CASE WHEN COALESCE(CAST({col} as int),0) = 0 THEN '' ELSE CONCAT('{col}:', PRINTF('%d', CAST({col} as int)), ' ') END\".format(col=colname)\n",
    "        elif coltype == 'string':\n",
    "            # Clean up all the spurious characters\n",
    "            return \"CASE WHEN COALESCE({col},'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE({col}, '[\\\\\\\\x00-\\\\\\\\x2a|\\\\\\\\x2c|\\\\\\\\x2f|\\\\\\\\x3a-\\\\\\\\x40|\\\\\\\\x5b-\\\\\\\\x5e|\\\\\\\\x60|\\\\\\\\x7b-\\\\\\\\x7f]', ''), ' ') END\".format(col=colname)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def __col_ns(self, namespaces):\n",
    "        \"\"\" Return the combined SQL for each particular VW namespace \n",
    "            This will be of the format \"|namespace FeatureA:123 FeatureB:0.0 SomeText\"\n",
    "        \"\"\"\n",
    "        featureset = []\n",
    "        for ns, cols in namespaces.iteritems():\n",
    "            features = \"\\n,\".join([self.__col_sql(c[0], c[1]) for c in cols ])\n",
    "            featureset.append(\"\\n'|{ns} '\\n, {fs}\".format(ns=ns, fs=features))\n",
    "        return ','.join(featureset)\n",
    "    \n",
    "    def __assemble_sql(self, cols, label_col, tag_col, src_table):\n",
    "        s = \"AND {}\".format(self.sample_sql) if self.sample_sql else \"\"\n",
    "        sql = \"\"\"\n",
    "            SELECT CONCAT(PRINTF('%.4f', COALESCE({label}, 0.0)), ' 1.0 ', {tag_col}, {cols} ) \n",
    "            FROM {table_name}\n",
    "            WHERE ds > ''\n",
    "                AND {label} IS NOT NULL\n",
    "                {sample}\n",
    "            \"\"\".format(label=label_col,\n",
    "                       cols=self.__col_ns(cols), \n",
    "                       tag_col = tag_col,\n",
    "                       table_name=src_table,\n",
    "                       sample=s\n",
    "                      )\n",
    "        return sql\n",
    "    \n",
    "    def create_table(self, table_name, table_location):\n",
    "        \"\"\" Create the destination table \"\"\"\n",
    "        hh = HiveCliHook(self.hive_conn_id)\n",
    "        create_sql = \"\"\"\n",
    "            DROP TABLE IF EXISTS {table_name};\n",
    "            CREATE EXTERNAL TABLE {table_name} (\n",
    "                input_line    STRING\n",
    "            )\n",
    "            STORED AS TEXTFILE\n",
    "            LOCATION '{location}';\n",
    "            ;\n",
    "        \"\"\".format(table_name=table_name, location=table_location)\n",
    "        hh.run_cli(create_sql)\n",
    "        \n",
    "    def __gen_sql(self):\n",
    "        select_sql = self.__assemble_sql(self.nsgroups,\n",
    "                                         self.label_column, \n",
    "                                         self.tag_column,\n",
    "                                         self.src_table)\n",
    "        \n",
    "        limit_sql = \"LIMIT {}\".format(self.limit) if self.limit else \"\"\n",
    "        \n",
    "        insert_sql = \"\"\"\n",
    "            SET hive.exec.compress.output=false;\n",
    "            SET mapred.reduce.tasks=10;\n",
    "            INSERT OVERWRITE TABLE {dst_table}\n",
    "            {select_sql}\n",
    "            {limit_sql}\n",
    "            ;\n",
    "        \"\"\".format(dst_table=self.dst_table, \n",
    "                   select_sql=select_sql,\n",
    "                   limit_sql=limit_sql)\n",
    "        \n",
    "        return(insert_sql)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" Compile the SQL and run it on Hive! \"\"\"\n",
    "        hh = HiveCliHook(self.hive_conn_id)\n",
    "       \n",
    "        self.create_table(self.dst_table, self.dst_hdfs_location)\n",
    "\n",
    "        finalsql = self.__gen_sql()\n",
    "        logging.info(finalsql)\n",
    "        hh.run_cli(finalsql)\n",
    "        logging.info(\"You should be able to find your files at {}\".format(self.dst_hdfs_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exclude these columns from the final result\n",
    "EXCLUDE_COLS = [\n",
    "    'ds',\n",
    "    'guest_id',\n",
    "    'part',\n",
    "    'randnum',\n",
    "    'sample01',\n",
    "    'sample1',\n",
    "    'sample10',\n",
    "    'sample25',\n",
    "    'sample35',\n",
    "    'sample50',\n",
    "    'target_p180',\n",
    "    'target_p28',\n",
    "    'target_p730',\n",
    "]\n",
    "\n",
    "# Create custom namespace mappings for columns\n",
    "# Anything not given a namespace will be mapped to \"other\"\n",
    "CUSTOM_NS = {\n",
    "   'dow': 'time',\n",
    "   'dsmonth': 'time',\n",
    "   'yr': 'time',\n",
    "   'dim_week_of_year': 'calendar',\n",
    "   'dim_year': 'calendar',\n",
    "   'dim_year': 'calendar',\n",
    "   'dim_city': 'location',\n",
    "   'dim_country': 'location',\n",
    "   'dim_lat': 'location',\n",
    "   'dim_lng': 'location',\n",
    "   'dim_city': 'location',\n",
    "   'dim_city': 'location',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-11-25 00:03:20,769] {base_hook.py:53} INFO - Using connection to: localhost\n",
      "[2016-11-25 00:03:20,825] {hive_hooks.py:131} INFO - hive -f /tmp/airflow_hiveop_IJ3qga/tmp5ChOS8\n",
      "[2016-11-25 00:03:23,215] {hive_hooks.py:145} INFO - 16/11/25 00:03:23 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore.\n",
      "[2016-11-25 00:03:23,293] {hive_hooks.py:145} INFO - \n",
      "[2016-11-25 00:03:23,294] {hive_hooks.py:145} INFO - Logging initialized using configuration in jar:file:/mnt/var/opt/CDH-5.3.3-1.cdh5.3.3.p1174.932/jars/hive-common-0.13.1-cdh5.3.3.jar!/hive-log4j.properties\n",
      "[2016-11-25 00:03:24,853] {hive_hooks.py:145} INFO - OK\n",
      "[2016-11-25 00:03:24,855] {hive_hooks.py:145} INFO - Time taken: 0.788 seconds\n",
      "[2016-11-25 00:03:24,978] {hive_hooks.py:145} INFO - OK\n",
      "[2016-11-25 00:03:24,979] {hive_hooks.py:145} INFO - Time taken: 0.123 seconds\n",
      "[2016-11-25 00:03:25,251] {hive_hooks.py:145} INFO - OK\n",
      "[2016-11-25 00:03:25,252] {hive_hooks.py:145} INFO - Time taken: 0.272 seconds\n",
      "[2016-11-25 00:03:25,586] {<ipython-input-24-9cd8850efe80>:146} INFO - \n",
      "            SET hive.exec.compress.output=false;\n",
      "            SET mapred.reduce.tasks=10;\n",
      "            INSERT OVERWRITE TABLE hamel.vw_input_data_trainS1_top50\n",
      "            \n",
      "            SELECT CONCAT(PRINTF('%.4f', COALESCE(target_p365, 0.0)), ' 1.0 ', guest_id, \n",
      "'|other '\n",
      ", CASE WHEN COALESCE(domain,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(domain, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(soft_guestfees_p28,0.0) = 0.0 THEN '' ELSE CONCAT('soft_guestfees_p28:', PRINTF('%.2f', soft_guestfees_p28), ' ') END\n",
      ",CASE WHEN COALESCE(reviews_og_m180,0.0) = 0.0 THEN '' ELSE CONCAT('reviews_og_m180:', PRINTF('%.2f', reviews_og_m180), ' ') END\n",
      ",CASE WHEN COALESCE(visits_m365,0.0) = 0.0 THEN '' ELSE CONCAT('visits_m365:', PRINTF('%.2f', visits_m365), ' ') END\n",
      ",CASE WHEN COALESCE(dim_state,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(dim_state, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(booking_value_m365,0.0) = 0.0 THEN '' ELSE CONCAT('booking_value_m365:', PRINTF('%.2f', booking_value_m365), ' ') END\n",
      ",CASE WHEN COALESCE(bookings,0.0) = 0.0 THEN '' ELSE CONCAT('bookings:', PRINTF('%.2f', bookings), ' ') END\n",
      ",CASE WHEN COALESCE(guest_fees,0.0) = 0.0 THEN '' ELSE CONCAT('guest_fees:', PRINTF('%.2f', guest_fees), ' ') END\n",
      ",CASE WHEN COALESCE(days_since_profilevisible,0) = 0 THEN '' ELSE CONCAT('days_since_profilevisible:', PRINTF('%d', days_since_profilevisible), ' ') END\n",
      ",CASE WHEN COALESCE(visits_m7,0.0) = 0.0 THEN '' ELSE CONCAT('visits_m7:', PRINTF('%.2f', visits_m7), ' ') END\n",
      ",CASE WHEN COALESCE(5star_overall_ratings,0.0) = 0.0 THEN '' ELSE CONCAT('5star_overall_ratings:', PRINTF('%.2f', 5star_overall_ratings), ' ') END\n",
      ",CASE WHEN COALESCE(nights_booked,0.0) = 0.0 THEN '' ELSE CONCAT('nights_booked:', PRINTF('%.2f', nights_booked), ' ') END\n",
      ",CASE WHEN COALESCE(booking_value,0.0) = 0.0 THEN '' ELSE CONCAT('booking_value:', PRINTF('%.2f', booking_value), ' ') END\n",
      ",CASE WHEN COALESCE(logins_web_m7,0.0) = 0.0 THEN '' ELSE CONCAT('logins_web_m7:', PRINTF('%.2f', logins_web_m7), ' ') END\n",
      ",CASE WHEN COALESCE(city,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(city, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(days_since_firstactive,0) = 0 THEN '' ELSE CONCAT('days_since_firstactive:', PRINTF('%d', days_since_firstactive), ' ') END\n",
      ",CASE WHEN COALESCE(visits_web_m365,0.0) = 0.0 THEN '' ELSE CONCAT('visits_web_m365:', PRINTF('%.2f', visits_web_m365), ' ') END\n",
      ",CASE WHEN COALESCE(logins_web,0.0) = 0.0 THEN '' ELSE CONCAT('logins_web:', PRINTF('%.2f', logins_web), ' ') END\n",
      ",CASE WHEN COALESCE(guest_fees_m28,0.0) = 0.0 THEN '' ELSE CONCAT('guest_fees_m28:', PRINTF('%.2f', guest_fees_m28), ' ') END\n",
      ",CASE WHEN COALESCE(bookings_m28,0.0) = 0.0 THEN '' ELSE CONCAT('bookings_m28:', PRINTF('%.2f', bookings_m28), ' ') END\n",
      ",CASE WHEN COALESCE(visits_web_m90,0.0) = 0.0 THEN '' ELSE CONCAT('visits_web_m90:', PRINTF('%.2f', visits_web_m90), ' ') END\n",
      ",CASE WHEN COALESCE(contacts_m28,0) = 0 THEN '' ELSE CONCAT('contacts_m28:', PRINTF('%d', contacts_m28), ' ') END\n",
      ",CASE WHEN COALESCE(booking_value_m180,0.0) = 0.0 THEN '' ELSE CONCAT('booking_value_m180:', PRINTF('%.2f', booking_value_m180), ' ') END\n",
      ",CASE WHEN COALESCE(nights_booked_m180,0.0) = 0.0 THEN '' ELSE CONCAT('nights_booked_m180:', PRINTF('%.2f', nights_booked_m180), ' ') END\n",
      ",CASE WHEN COALESCE(visits_web,0.0) = 0.0 THEN '' ELSE CONCAT('visits_web:', PRINTF('%.2f', visits_web), ' ') END\n",
      ",CASE WHEN COALESCE(visits_m90,0.0) = 0.0 THEN '' ELSE CONCAT('visits_m90:', PRINTF('%.2f', visits_m90), ' ') END\n",
      ",CASE WHEN COALESCE(guest_fees_m90,0.0) = 0.0 THEN '' ELSE CONCAT('guest_fees_m90:', PRINTF('%.2f', guest_fees_m90), ' ') END\n",
      ",CASE WHEN COALESCE(booking_value_m90,0.0) = 0.0 THEN '' ELSE CONCAT('booking_value_m90:', PRINTF('%.2f', booking_value_m90), ' ') END\n",
      ",CASE WHEN COALESCE(nights_booked_m365,0.0) = 0.0 THEN '' ELSE CONCAT('nights_booked_m365:', PRINTF('%.2f', nights_booked_m365), ' ') END\n",
      ",CASE WHEN COALESCE(bookings_m180,0.0) = 0.0 THEN '' ELSE CONCAT('bookings_m180:', PRINTF('%.2f', bookings_m180), ' ') END\n",
      ",CASE WHEN COALESCE(guest_fees_m180,0.0) = 0.0 THEN '' ELSE CONCAT('guest_fees_m180:', PRINTF('%.2f', guest_fees_m180), ' ') END\n",
      ",CASE WHEN COALESCE(days_since_firstbooking,0) = 0 THEN '' ELSE CONCAT('days_since_firstbooking:', PRINTF('%d', days_since_firstbooking), ' ') END\n",
      ",CASE WHEN COALESCE(logins_m28,0.0) = 0.0 THEN '' ELSE CONCAT('logins_m28:', PRINTF('%.2f', logins_m28), ' ') END\n",
      ",CASE WHEN COALESCE(visits_web_m7,0.0) = 0.0 THEN '' ELSE CONCAT('visits_web_m7:', PRINTF('%.2f', visits_web_m7), ' ') END\n",
      ",CASE WHEN COALESCE(visits_m180,0.0) = 0.0 THEN '' ELSE CONCAT('visits_m180:', PRINTF('%.2f', visits_m180), ' ') END\n",
      ",CASE WHEN COALESCE(language,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(language, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(country,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(country, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(contacts_m7,0) = 0 THEN '' ELSE CONCAT('contacts_m7:', PRINTF('%d', contacts_m7), ' ') END\n",
      ",CASE WHEN COALESCE(logins_m7,0.0) = 0.0 THEN '' ELSE CONCAT('logins_m7:', PRINTF('%.2f', logins_m7), ' ') END\n",
      ",CASE WHEN COALESCE(visits_web_m28,0.0) = 0.0 THEN '' ELSE CONCAT('visits_web_m28:', PRINTF('%.2f', visits_web_m28), ' ') END\n",
      ",CASE WHEN COALESCE(contacts_m180,0) = 0 THEN '' ELSE CONCAT('contacts_m180:', PRINTF('%d', contacts_m180), ' ') END\n",
      ",CASE WHEN COALESCE(gender,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(gender, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(booking_value_m28,0.0) = 0.0 THEN '' ELSE CONCAT('booking_value_m28:', PRINTF('%.2f', booking_value_m28), ' ') END\n",
      ",CASE WHEN COALESCE(guest_fees_m365,0.0) = 0.0 THEN '' ELSE CONCAT('guest_fees_m365:', PRINTF('%.2f', guest_fees_m365), ' ') END\n",
      ",CASE WHEN COALESCE(visits_m28,0.0) = 0.0 THEN '' ELSE CONCAT('visits_m28:', PRINTF('%.2f', visits_m28), ' ') END\n",
      ",CASE WHEN COALESCE(bookings_m365,0.0) = 0.0 THEN '' ELSE CONCAT('bookings_m365:', PRINTF('%.2f', bookings_m365), ' ') END\n",
      ",CASE WHEN COALESCE(nights_booked_m90,0.0) = 0.0 THEN '' ELSE CONCAT('nights_booked_m90:', PRINTF('%.2f', nights_booked_m90), ' ') END\n",
      ",CASE WHEN COALESCE(bookings_m90,0.0) = 0.0 THEN '' ELSE CONCAT('bookings_m90:', PRINTF('%.2f', bookings_m90), ' ') END,\n",
      "'|time '\n",
      ", CASE WHEN COALESCE(dsmonth,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(dsmonth, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(yr,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(yr, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END\n",
      ",CASE WHEN COALESCE(dow,'') = '' THEN '' ELSE CONCAT(REGEXP_REPLACE(dow, '[\\\\x00-\\\\x2a|\\\\x2c|\\\\x2f|\\\\x3a-\\\\x40|\\\\x5b-\\\\x5e|\\\\x60|\\\\x7b-\\\\x7f]', ''), ' ') END ) \n",
      "            FROM hamel.ltv_train_s1_top50\n",
      "            WHERE ds > ''\n",
      "                AND target_p365 IS NOT NULL\n",
      "                \n",
      "            \n",
      "            \n",
      "            ;\n",
      "        \n",
      "[2016-11-25 00:03:25,587] {hive_hooks.py:131} INFO - hive -f /tmp/airflow_hiveop_iTdQpn/tmpRtRFtr\n",
      "[2016-11-25 00:03:28,010] {hive_hooks.py:145} INFO - 16/11/25 00:03:28 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore.\n",
      "[2016-11-25 00:03:28,089] {hive_hooks.py:145} INFO - \n",
      "[2016-11-25 00:03:28,090] {hive_hooks.py:145} INFO - Logging initialized using configuration in jar:file:/mnt/var/opt/CDH-5.3.3-1.cdh5.3.3.p1174.932/jars/hive-common-0.13.1-cdh5.3.3.jar!/hive-log4j.properties\n",
      "[2016-11-25 00:03:29,683] {hive_hooks.py:145} INFO - OK\n",
      "[2016-11-25 00:03:29,685] {hive_hooks.py:145} INFO - Time taken: 0.813 seconds\n",
      "[2016-11-25 00:03:30,837] {hive_hooks.py:145} INFO - Total jobs = 3\n",
      "[2016-11-25 00:03:30,853] {hive_hooks.py:145} INFO - Launching Job 1 out of 3\n",
      "[2016-11-25 00:03:30,858] {hive_hooks.py:145} INFO - Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "[2016-11-25 00:03:34,795] {hive_hooks.py:145} INFO - Starting Job = job_1478879867775_605439, Tracking URL = http://i-faa9b805.inst.aws.airbnb.com:8088/proxy/application_1478879867775_605439/\n",
      "[2016-11-25 00:03:34,796] {hive_hooks.py:145} INFO - Kill Command = /mnt/var/opt/CDH-5.3.3-1.cdh5.3.3.p1174.932/lib/hadoop/bin/hadoop job  -kill job_1478879867775_605439\n",
      "[2016-11-25 00:03:53,599] {hive_hooks.py:145} INFO - Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 0\n",
      "[2016-11-25 00:03:53,673] {hive_hooks.py:145} INFO - 2016-11-25 00:03:53,668 Stage-1 map = 0%,  reduce = 0%\n",
      "[2016-11-25 00:04:52,989] {hive_hooks.py:145} INFO - 2016-11-25 00:04:52,989 Stage-1 map = 9%,  reduce = 0%, Cumulative CPU 1153.33 sec\n"
     ]
    }
   ],
   "source": [
    "htvw = HiveToVWInput(\n",
    "    'hamel.ltv_train_s1_top50', # Input table name\n",
    "    'hamel.vw_input_data_trainS1_top50', # Output table name\n",
    "    '/user/hive/warehouse/hamel.db/vw_input_data_trainS1_top50', # HDFS File location\n",
    "    'target_p365', # Label column\n",
    "    'guest_id', #tag column\n",
    "    #limit=1000000000000, # Limit the number of rows from the source table\n",
    "    sample_sql=\"\", # This will be added to the where clause for sampling \n",
    "    excludes=EXCLUDE_COLS, # column exclude list\n",
    "    custom_namespaces=CUSTOM_NS, # Custom mapping for VW namespaces\n",
    "    )\n",
    "htvw.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "htvw_val = HiveToVWInput(\n",
    "    'hamel.ltv_validation_top50', # Input table name\n",
    "    'hamel.vw_input_data_validation_top50 ', # Output table name\n",
    "    '/user/hive/warehouse/hamel.db/vw_input_data_validation_top50', # HDFS File location\n",
    "    'target_p365', # Label column\n",
    "    'guest_id', #tag column\n",
    "    #limit=1000000000000, # Limit the number of rows from the source table\n",
    "    sample_sql=\"\", # This will be added to the where clause for sampling \n",
    "    excludes=EXCLUDE_COLS, # column exclude list\n",
    "    custom_namespaces=CUSTOM_NS, # Custom mapping for VW namespaces\n",
    "    )\n",
    "\n",
    "htvw_val.run()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
